# Solu√ß√£o para o desafio 

## üß† Como usar o projeto

### üõ†Ô∏è Pr√©-requisitos

Certifique-se de ter o **Python 3.13** instalado.
Depois, instale todas as depend√™ncias necess√°rias com:

```bash
pip install -r requirements.txt
```

---
Certifique-se de criar o arquivo 
```bash
.env
```
e adicionar a sua api_key da openai
```bash
OPENAI_API_KEY = sk-...
```
### üöÄ Executando a aplica√ß√£o

A interface do projeto foi constru√≠da com **Streamlit**, permitindo o uso de forma simples e intuitiva ‚Äî tanto para o processamento de **um √∫nico PDF** quanto para **v√°rios arquivos em lote**.

Para iniciar a aplica√ß√£o, execute no terminal:

```bash
streamlit run app.py
```

Isso abrir√° automaticamente a interface web no navegador.
A partir dela, voc√™ poder√°:

* Fazer **upload de um PDF** e de seu **label/schema em formato JSON**;
* Obter a sa√≠da estruturada com os campos extra√≠dos;
* Alternar para o modo **batch**, selecionando uma **pasta contendo v√°rios PDFs** e um **dataset.json** com os labels correspondentes.

---

### üß© Fun√ß√£o principal: `estate_machine()`

Toda a orquestra√ß√£o do pipeline est√° concentrada na fun√ß√£o:

```python
def state_machine(dataset_path: str,
                  nome_da_pasta_dos_pdfs: str,
                  on_step=None):
```

Ela √© respons√°vel por:

* Ler o schema JSON e o arquivo PDF;
* Executar as etapas de pr√©-processamento (como agrupamento de linhas e verifica√ß√£o de similaridade);
* Decidir, de forma adaptativa, se o resultado pode ser reutilizado do cache ou se √© necess√°rio invocar o modelo LLM;
* Retornar o JSON final com as informa√ß√µes estruturadas conforme o schema solicitado.

Essa fun√ß√£o representa o **n√∫cleo l√≥gico do sistema** e garante o equil√≠brio entre custo, velocidade e precis√£o.

---

## ‚öôÔ∏è Otimiza√ß√£o e Estrat√©gia de *Hard Response*

Uma das principais preocupa√ß√µes deste projeto foi equilibrar **tempo de execu√ß√£o, custo e precis√£o**, especialmente considerando que m√∫ltiplas chamadas √† API da OpenAI tornam o processo caro e lento.
Para mitigar esse problema, foi desenvolvida a rotina **`hard_response()`**, inspirada em mecanismos de **treinamento em batch** utilizados em *Machine Learning*.

---

### üéØ Objetivo

A ideia central foi **aproveitar o comportamento repetitivo de PDFs com layout semelhante**.
Enquanto documentos de um mesmo *label* compartilham padr√µes estruturais, a IA pode cometer os mesmos acertos e erros de forma consistente.
Assim, foi criada uma heur√≠stica que **aprende com as respostas anteriores** e **replica as extra√ß√µes bem-sucedidas**, reduzindo drasticamente o n√∫mero de chamadas ao modelo LLM.

---

### üß† Funcionamento

1. **Mem√≥ria Cache:**
   Cada solicita√ß√£o √© armazenada em uma estrutura de mem√≥ria (`memoria_cache`), contendo tanto o *label* quanto as coordenadas e respostas obtidas.

2. **Verifica√ß√£o de Similaridade:**
   Quando um novo PDF √© processado, o sistema compara seu layout e regi√µes de texto com documentos j√° vistos.
   Caso sejam **suficientemente semelhantes**, o sistema identifica **regi√µes de coordenadas pr√≥ximas** e tenta **extrair os campos diretamente** dessas √°reas, sem invocar o LLM.

3. **Hard Response:**
   Se a semelhan√ßa ultrapassa o limiar definido, a fun√ß√£o `hard_response()` √© acionada ‚Äî aplicando diretamente as respostas armazenadas anteriormente.
   Essa t√©cnica mostrou-se eficaz, sendo capaz de **substituir com sucesso cerca de 60% das chamadas ao modelo**, garantindo custo reduzido e resposta quase instant√¢nea.

---

### ‚öñÔ∏è Trade-offs e Decis√µes de Projeto

Essa abordagem √© uma **faca de dois gumes**:

* Se o modelo estava **acertando 60% das vezes**, o *hard response* passa a acertar esses casos **100% das vezes**, consolidando o acerto.
* Por√©m, **erros acurados tamb√©m s√£o propagados**.

Por isso, foi feito um investimento cuidadoso em **Prompt Engineering**, buscando **instru√ß√µes altamente espec√≠ficas e contextuais** que aumentassem a taxa de acerto natural do modelo antes mesmo do uso de cache.

Optou-se por **n√£o ampliar o hard_response com heur√≠sticas estat√≠sticas complexas**, mantendo a estrat√©gia **conservadora e barata por itera√ß√£o**, priorizando efici√™ncia e reprodutibilidade.

---

### üß© Estrat√©gias Avaliadas e Decis√µes

* ‚ùå **Bag of Words:**
  Descartada. Apesar de √∫til para an√°lise textual, exigiria contextos amplos para cada chave, gerando sobreposi√ß√£o de tokens e custo elevado.

* ‚ùå **Chamada individual por chave:**
  Tamb√©m descartada. Muitas chaves n√£o possuem correla√ß√£o direta no texto, o que obrigaria enviar o documento completo repetidas vezes.

* ‚úÖ **Prompt Engineering refinado:**
  Adotado como estrat√©gia principal.
  Dada a natureza dos PDFs (apenas uma p√°gina), foi poss√≠vel alcan√ßar excelente desempenho apenas com ajustes no prompt e controle de contexto.

---

### üí° Conclus√£o

A fun√ß√£o `hard_response()` atua como um **mecanismo de mem√≥ria inteligente**, replicando o comportamento de aprendizado incremental de modelos de ML sem custo adicional de infer√™ncia.
Essa solu√ß√£o, combinada com **prompt engineering e cache adaptativo**, resultou em uma arquitetura **r√°pida, barata e robusta**, capaz de lidar com varia√ß√µes de layout mantendo alta precis√£o.

---

## üîÑ Solu√ß√£o Avan√ßada: Pipeline em Malha de Controle

Durante o desenvolvimento, considerei uma solu√ß√£o mais sofisticada, inspirada em **sistemas de controle moderno**, que embora promissora, ultrapassava o escopo temporal do desafio (uma semana).

A ideia seria estruturar o processo de extra√ß√£o em uma **malha de controle fechada**, na qual o sistema **avaliaria e corrigiria iterativamente suas pr√≥prias sa√≠das** ‚Äî uma analogia direta a um **controlador PID/PNL** (Proporcional‚ÄìN√£o Linear) ajustando o erro entre o estado atual e o desejado.

---

### üß† Conceito

Nessa arquitetura, os **outputs anteriores** seriam **realimentados como inputs** para um **algoritmo de avalia√ß√£o PNL (Programa√ß√£o Neurolingu√≠stica / heur√≠stica de controle n√£o linear)**.
Esse m√≥dulo compararia as respostas das chaves extra√≠das entre si, identificando **discrep√¢ncias sem√¢nticas ou inconsist√™ncias l√≥gicas** (por exemplo, diverg√™ncias entre nome e g√™nero, ou se a categoria declarada contradiz o tipo de documento).

O sistema faria ent√£o um **‚Äúju√≠zo de valor‚Äù** sobre a adequa√ß√£o de cada campo, ajustando as respostas dentro de uma **pipeline iterativa em malha fechada** ‚Äî refinando progressivamente o resultado at√© convergir para a extra√ß√£o mais coerente e est√°vel.

---

### ‚öôÔ∏è Funcionamento Proposto

1. **Extra√ß√£o inicial (malha aberta):**
   O LLM executa a primeira extra√ß√£o com base no schema e texto bruto.

2. **Compara√ß√£o e avalia√ß√£o (malha de realimenta√ß√£o):**
   O m√≥dulo de controle avalia a consist√™ncia interna das respostas e as compara com padr√µes aprendidos de PDFs similares.

3. **Corre√ß√£o e reentrada:**
   Caso identifique inconsist√™ncias, o sistema ajusta localmente os valores (proporcional e n√£o linearmente), retroalimentando o resultado na pipeline ‚Äî sem necessidade de nova chamada √† API da OpenAI.

4. **Converg√™ncia:**
   O processo se repete at√© o erro m√©dio (diferen√ßa sem√¢ntica entre chaves correlatas) atingir um limite aceit√°vel.

---

### üí∞ Custo e Trade-off

Implementar essa abordagem aumentaria o custo da solu√ß√£o ‚Äî tanto em termos computacionais quanto de engenharia.
Apesar de reduzir drasticamente a depend√™ncia de chamadas externas ao LLM, exigiria:

* Calibra√ß√£o do m√≥dulo PNL local;
* Cria√ß√£o de m√©tricas sem√¢nticas de erro espec√≠ficas por tipo de campo;
* Mecanismo de retroalimenta√ß√£o controlada para garantir estabilidade da malha.

Por isso, optei por **n√£o implement√°-la nesta fase**, priorizando uma solu√ß√£o **eficiente, est√°vel e de baixo custo**, mas a arquitetura atual foi constru√≠da de forma que **essa extens√£o possa ser integrada futuramente**.

---

### üß© Potencial Futuro

A aplica√ß√£o de **conceitos de controle em pipelines de IA** abre caminho para solu√ß√µes mais aut√¥nomas e autoavaliativas ‚Äî verdadeiros **sistemas de extra√ß√£o com autoconfian√ßa**.
No contexto deste projeto, essa integra√ß√£o poderia oferecer um **‚Äúfine-tuning iterativo local‚Äù** sem aumentar o n√∫mero de chamadas √†s APIs, transformando o sistema em uma **malha adaptativa de autovalida√ß√£o sem√¢ntica**.

---


